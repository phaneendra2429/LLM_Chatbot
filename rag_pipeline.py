# -*- coding: utf-8 -*-
"""rag_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E9mXhYwW-qlnZY5LhxginPzdmLGBm4wt

## Install Libraries
"""

!pip install --upgrade langchain

!pip install pymupdf
!pip install langchain_community
!pip install sentence-transformers
!pip install chromadb

"""## Import Libraries"""

from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings.sentence_transformer import (
    SentenceTransformerEmbeddings,
)
from langchain.retrievers import ParentDocumentRetriever
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain.storage import InMemoryStore

"""## Environment Variables"""

HUGGINGFACEHUB_API_TOKEN = "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"

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

"""## Retriever"""

loader = PyMuPDFLoader("DepressionGuide-web.pdf")

documents  = loader.load()

# split it into chunks
# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)

# docs = text_splitter.split_documents(documents)

# create the open-source embedding function
embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# load it into Chroma
# db = Chroma.from_documents(docs, embedding_function)

# This text splitter is used to create the parent documents
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)

# This text splitter is used to create the child documents
# It should create documents smaller than the parent
child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# The vectorstore to use to index the child chunks
vectorstore = Chroma(
    collection_name="split_parents", embedding_function= embedding_function
)
# The storage layer for the parent documents
store = InMemoryStore()

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

retriever.add_documents(documents)

retriever.get_relevant_documents("I'm Tired all the time, feeling “lazy”")

# query it
# query = "How To Treat Depression?"
# docs = db.similarity_search(query)

# Step 1: Retrieve
# retriever = db.as_retriever()

retriever

"""## Augmentor"""

# # Step 2: Augment
# template = """You are an assistant for question-answering tasks.
# Use the following pieces of retrieved context to answer the question.
# If you don't know the answer, just say that you don't know.
# Use three sentences maximum and keep the answer concise.
# Question: {question}
# Context: {context}
# Answer:
# """
# prompt = ChatPromptTemplate.from_template(template)

# print(prompt)

from langchain import PromptTemplate
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# Define system and user message templates
system_message_template = '''You are a Mental Health Specialist (therapist).
Your job is to provide support for individuals with Depressive Disorder.
Act as a compassionate listener and offer helpful responses based on the user's queries.
If the user seeks casual conversation, be friendly and supportive.
If they seek factual information, use the context of the conversation to provide relevant responses.
If unsure, be honest and say, 'This is out of the scope of my knowledge.' Always respond directly to the user's query without deviation.
Context: {context} '''

user_message_template = "User Query: {question} Answer:"

# Create message templates
system_message = SystemMessagePromptTemplate.from_template(system_message_template)
user_message = HumanMessagePromptTemplate.from_template(user_message_template)

# Compile messages into a chat prompt template
messages = [system_message, user_message]
chatbot_prompt = ChatPromptTemplate.from_messages(messages)

chatbot_prompt

custom_template = """Given the following conversation and a follow-up message, \
rephrase the follow-up message to a stand-alone question or instruction that \
represents the user's intent, add all context needed if necessary to generate a complete and \
unambiguous question or instruction, only based on the history, don't make up messages. \
Maintain the same language as the follow up input message.

Chat History:
{chat_history}

Follow Up Input: {question}
Standalone question or instruction:"""

"""## Generator"""

# "HuggingFaceH4/zephyr-7b-beta"

from langchain_community.llms import HuggingFaceHub

llm = HuggingFaceHub(
    repo_id="mistralai/Mistral-7B-v0.1",

    task="text-generation",
    model_kwargs={
        "max_new_tokens": 200,
        "top_k": 30,
        "temperature": 0.1,
        "repetition_penalty": 2.0,
    },
)

# from langchain_community.llms import CTransformers
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# config={'max_new_tokens': 100,
#                               'temperature': 0.05,
#                               'context_length': 500, 'stream': True,'repetition_penalty':2}
# llm = CTransformers(model='TheBloke/Llama-2-7B-GGML',callbacks=[StreamingStdOutCallbackHandler()], model_file='llama-2-7b.ggmlv3.q5_0.bin',config=config)

"""## Pipeline"""

# Step 3: Generate

# from langchain.chat_models import ChatOpenAI
# from langchain.schema.runnable import RunnablePassthrough
# from langchain.schema.output_parser import StrOutputParser

# # llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# rag_chain = (
#     {"context": retriever,  "question": RunnablePassthrough()}
#     | prompt
#     | llm
#     | StrOutputParser()
# )

# query = "How To Treat Depression?"
# completion = rag_chain.invoke(query)

# completion.split("Answer")[0]

"""## Orchestrator"""

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# # llm = OpenAI(temperature=0)
# conversation = ConversationChain(
#     llm=llm,
#     verbose=True,
#     memory=ConversationBufferMemory()
# )

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ChatMessageHistory,ConversationSummaryBufferMemory

# Assuming you already have defined vectordb and llm
# retriever = retriever.as_retriever(search_type="mmr",search_kwargs={"k": 2})

# Provide the chat history when initializing the ConversationalRetrievalChain
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory = ConversationSummaryBufferMemory(
        memory_key="chat_history",
        input_key="question",
        llm=llm,
        max_token_limit=40,
        return_messages=True
    ),
    return_source_documents=False,
    chain_type="stuff",
    max_tokens_limit=100,
    condense_question_prompt= PromptTemplate.from_template(custom_template),
    combine_docs_chain_kwargs={'prompt': chatbot_prompt},
    verbose=True,
    return_generated_question=False,
)

history = ChatMessageHistory()

def ask(question: str):
    answer = qa({"question": question,"chat_history":history.messages})["answer"]
    print("##------##")
    print(answer)
    return answer

ask("I'm Tired all the time, feeling “lazy”")

ask("Not Want To Do Anything?")

"""## Analyzer

### Summarizer
"""

# print(conversation.memory.buffer)



"""### Categorizer"""

