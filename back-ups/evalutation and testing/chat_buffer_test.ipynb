{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# LLM Generator\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "config = {'max_new_tokens': 256,\n",
    "          'temperature': 0.4,\n",
    "          'repetition_penalty': 1.1,\n",
    "          'context_length': 4096, # Set to max for Chat Summary, Llama-2 has a max context length of 4096\n",
    "          }\n",
    "\n",
    "llm = CTransformers(model='W:\\\\Projects\\\\LangChain\\\\models\\\\quantizedGGUF-theBloke\\\\llama-2-7b-chat.Q2_K.gguf',\n",
    "                    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "                    config=config,\n",
    "                    gpu_layers = 25)\n",
    "\n",
    "# Implement another function to pass an array of PDFs / CSVs / Excels\n",
    "from rag_pipeline import instantiate_rag\n",
    "retriever = instantiate_rag()\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "# Docs:- https://python.langchain.com/docs/integrations/chat/llama2_chat\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "\n",
    "# Define system and user message templates\n",
    "with open('.\\\\prompts\\\\system_message_template.txt', 'r') as file:\n",
    "            system_message_template = file.read().replace('\\n', '')\n",
    "\n",
    "template_messages = [\n",
    "    SystemMessage(content=system_message_template),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(template_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama2Chat(llm=llm)\n",
    "memory = ConversationBufferMemory(llm=llm, memory_key=\"chat_history\", return_messages=True, max_token_limit=10000)\n",
    "chain = LLMChain(llm=model, prompt=prompt_template, memory=memory, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_responses = ['Nothing logged yet']\n",
    "ai_responses = ['Nothing logged yet']\n",
    "\n",
    "# Decide wether to place this in streamlit.py\n",
    "# or make a new post_process.py and import that to streamlit\n",
    "def extract_dialogues(text):\n",
    "    '''\n",
    "    returns a two lists for human and ai dialogues,\n",
    "    '''\n",
    "    human_dialogues = []\n",
    "    ai_dialogues = []\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Iterate through each line\n",
    "    for line in lines:\n",
    "        # Remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Check if the line starts with 'Human:' or 'AI:'\n",
    "        if line.startswith('Human:'):\n",
    "            # Extract the text after 'Human:'\n",
    "            human_dialogues.append(line[len('Human:'):].strip())\n",
    "        elif line.startswith('AI:'):\n",
    "            # Extract the text after 'AI:'\n",
    "            ai_dialogues.append(line[len('AI:'):].strip())\n",
    "    return human_dialogues, ai_dialogues\n",
    "\n",
    "def llm_generation(question):\n",
    "    global human_responses, ai_responses\n",
    "    print(chain.invoke(input=question))\n",
    "    human_responses, ai_responses = extract_dialogues(memory.buffer_as_str)\n",
    "    return 'Generation complete'\n",
    "\n",
    "def update_list():\n",
    "    global human_responses, ai_responses\n",
    "    human_responses, ai_responses = extract_dialogues(memory.buffer_as_str)\n",
    "    return 'responses updated'  \n",
    "\n",
    "def is_depressed():\n",
    "    # Implement Classification\n",
    "    all_user_inputs = ''.join(human_responses)\n",
    "    from nlp_models import sentiment_class, pattern_classification, corelation_analysis\n",
    "    is_depressed = sentiment_class(all_user_inputs)\n",
    "    return 'Not so depressed' if is_depressed[0][1] > 0.5 else 'is_depressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '  \"Congratulations on solving that problem! It can be very relieving and empowering when we overcome challenges. How are you feeling now?\"', 'chat_history': [HumanMessage(content='awesome, what do you think about todays day'), AIMessage(content='  \"I\\'m here to listen and offer support. Today can be tough for many reasons. How are you feeling about it?\"'), HumanMessage(content='I think the day is great'), AIMessage(content='  \"That\\'s great to hear! Can you tell me more about what\\'s making today great for you?\"'), HumanMessage(content='I think i just solved a critical problem'), AIMessage(content='  \"Congratulations on solving that problem! It can be very relieving and empowering when we overcome challenges. How are you feeling now?\"')]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Human: awesome, what do you think about todays day\\nAI:   \"I\\'m here to listen and offer support. Today can be tough for many reasons. How are you feeling about it?\"\\nHuman: I think the day is great\\nAI:   \"That\\'s great to hear! Can you tell me more about what\\'s making today great for you?\"\\nHuman: I think i just solved a critical problem\\nAI:   \"Congratulations on solving that problem! It can be very relieving and empowering when we overcome challenges. How are you feeling now?\"'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_generation('I think i just solved a critical problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: awesome, what do you think about todays day\\nAI:   \"I\\'m here to listen and offer support. Today can be tough for many reasons. How are you feeling about it?\"\\nHuman: I think the day is great\\nAI:   \"That\\'s great to hear! Can you tell me more about what\\'s making today great for you?\"\\nHuman: I think i just solved a critical problem\\nAI:   \"Congratulations on solving that problem! It can be very relieving and empowering when we overcome challenges. How are you feeling now?\"'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer_as_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Congratulations on solving that problem! It can be very relieving and empowering when we overcome challenges. How are you feeling now?\"'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_responses[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
